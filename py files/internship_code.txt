# # Install required packages
!pip install pytorch-lightning hydra-core scikit-image mahotas opencv-python-headless pandas

import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from pytorch_lightning import LightningModule, Trainer, seed_everything
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                           precision_score, recall_score, f1_score, roc_auc_score)
from PIL import Image
import cv2
from skimage.feature import local_binary_pattern, graycomatrix, graycoprops
from skimage.filters import gabor
from skimage import measure
import mahotas

import time
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


# Set seeds for reproducibility
SEED = 42
seed_everything(SEED, workers=True)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Enhanced Dataset Class with better error handling
class ClassifierDataset(Dataset):
    def __init__(self, annotations_path, transform=None, augment=False):
        self.annotations_path = annotations_path
        self.transform = transform
        self.augment = augment

        with open(annotations_path, "r") as f:
            data = json.load(f)

        self.images_list = data.get("images", [])
        self.annotations_list = data.get("annotations", [])
        self.categories_list = data.get("categories", [])

        self.images = {img["id"]: img for img in self.images_list}
        self.categories = {cat["id"]: idx for idx, cat in enumerate(self.categories_list)}

        self.valid_annotations = []
        for ann in self.annotations_list:
            bbox = ann.get("bbox", [0, 0, 0, 0])
            if bbox != [0, 0, 0, 0] and bbox[2] > 0 and bbox[3] > 0:
                if ann["image_id"] in self.images:
                    self.valid_annotations.append(ann)

        print(f"Dataset loaded: {len(self.valid_annotations)} valid annotations")

    def __len__(self):
        return len(self.valid_annotations)

    def __getitem__(self, idx):
        ann = self.valid_annotations[idx]
        img_meta = self.images[ann["image_id"]]

        img_filename = img_meta.get("file_name")
        images_dir = "./datasets/oral1/"
        full_path = os.path.join(images_dir, img_filename)

        try:
            image = Image.open(full_path).convert("RGB")
        except FileNotFoundError:
            if img_meta.get("path"):
                alt_path = "./" + img_meta["path"]
                image = Image.open(alt_path).convert("RGB")
            else:
                raise FileNotFoundError(f"Could not find image: {full_path}")

        x, y, w, h = ann["bbox"]
        x = max(0, min(x, image.width - 1))
        y = max(0, min(y, image.height - 1))
        w = max(1, min(w, image.width - x))
        h = max(1, min(h, image.height - y))

        subimage = image.crop((x, y, x + w, y + h))

        if self.transform:
            subimage = self.transform(subimage)

        category_id = ann.get("category_id")
        label = self.categories.get(category_id)

        return subimage, label

    def get_category_names(self):
        category_names = [None] * len(self.categories_list)
        for cat in self.categories_list:
            idx = self.categories[cat["id"]]
            category_names[idx] = cat["name"]
        return category_names

# Enhanced Texture Feature Extractor with more features
class EnhancedTextureFeatureExtractor:
    def __init__(self):
        self.lbp_radii = [1, 2, 3]  # Multiple radii for multi-scale LBP

    def extract_multiscale_lbp(self, gray_image):
        """Extract multi-scale LBP features"""
        features = []
        for radius in self.lbp_radii:
            n_points = 8 * radius
            lbp = local_binary_pattern(gray_image, n_points, radius, method='uniform')
            hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, range=(0, n_points + 2))
            hist = hist.astype(float)
            hist /= (hist.sum() + 1e-7)
            features.extend(hist)
        return np.array(features)

    def extract_enhanced_glcm(self, gray_image):
        """Extract enhanced GLCM features"""
        gray_image = (gray_image / 16).astype(np.uint8)  # 16 levels for better granularity

        distances = [1, 2, 3, 5]
        angles = [0, 45, 90, 135]
        features = []

        for distance in distances:
            for angle in angles:
                glcm = graycomatrix(gray_image, [distance], [np.radians(angle)],
                                 levels=16, symmetric=True, normed=True)

                contrast = graycoprops(glcm, 'contrast')[0, 0]
                dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]
                homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]
                energy = graycoprops(glcm, 'energy')[0, 0]
                correlation = graycoprops(glcm, 'correlation')[0, 0]
                asm = graycoprops(glcm, 'ASM')[0, 0]

                features.extend([contrast, dissimilarity, homogeneity, energy, correlation, asm])

        return np.array(features)

    def extract_gabor_features(self, gray_image):
        """Extract Gabor filter features with more frequencies"""
        frequencies = [0.05, 0.1, 0.2, 0.3, 0.4]
        angles = [0, 30, 60, 90, 120, 150]
        features = []

        for frequency in frequencies:
            for angle in angles:
                filt_real, filt_imag = gabor(gray_image, frequency=frequency, theta=np.radians(angle))
                features.extend([
                    filt_real.mean(), filt_real.var(),
                    filt_imag.mean(), filt_imag.var(),
                    np.sqrt(filt_real**2 + filt_imag**2).mean()  # Magnitude
                ])

        return np.array(features)

    def extract_haralick_features(self, gray_image):
        """Extract extended Haralick features"""
        try:
            haralick_features = mahotas.features.haralick(gray_image.astype(np.uint8), compute_14th_feature=True)
            return np.concatenate([haralick_features.mean(axis=0), haralick_features.std(axis=0)])
        except:
            return np.zeros(28)  # 14 mean + 14 std features

    def extract_statistical_features(self, gray_image):
        """Extract statistical features"""
        features = [
            gray_image.mean(),
            gray_image.std(),
            np.median(gray_image),
            gray_image.min(),
            gray_image.max(),
            np.percentile(gray_image, 25),
            np.percentile(gray_image, 75),
            measure.shannon_entropy(gray_image)
        ]
        return np.array(features)

    def extract_all_features(self, image_tensor):
        """Extract all texture features"""
        if isinstance(image_tensor, torch.Tensor):
            image_np = image_tensor.permute(1, 2, 0).numpy()
        else:
            image_np = np.array(image_tensor)

        if len(image_np.shape) == 3:
            gray_image = cv2.cvtColor((image_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
        else:
            gray_image = (image_np * 255).astype(np.uint8)

        # Extract all feature types
        lbp_features = self.extract_multiscale_lbp(gray_image)
        glcm_features = self.extract_enhanced_glcm(gray_image)
        gabor_features = self.extract_gabor_features(gray_image)
        haralick_features = self.extract_haralick_features(gray_image)
        statistical_features = self.extract_statistical_features(gray_image)

        all_features = np.concatenate([
            lbp_features, glcm_features, gabor_features,
            haralick_features, statistical_features
        ])

        return torch.tensor(all_features, dtype=torch.float32)

# Enhanced Fusion Dataset
class EnhancedFusionDataset(Dataset):
    def __init__(self, annotations_path, transform=None):
        self.base_dataset = ClassifierDataset(annotations_path, transform)
        self.texture_extractor = EnhancedTextureFeatureExtractor()

    def __len__(self):
        return len(self.base_dataset)

    def __getitem__(self, idx):
        image, label = self.base_dataset[idx]
        texture_features = self.texture_extractor.extract_all_features(image)
        return image, texture_features, label

    def get_category_names(self):
        return self.base_dataset.get_category_names()

# Enhanced CNN Feature Extractor with multiple backbones
class EnhancedCNNFeatureExtractor(nn.Module):
    def __init__(self, model_name="efficientnet_b0", output_dim=256):
        super().__init__()

        if model_name == "efficientnet_b0":
            weights = torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1
            self.backbone = torchvision.models.efficientnet_b0(weights=weights)
            in_features = self.backbone.classifier[1].in_features
            self.backbone.classifier = nn.Identity()
            self.preprocess = weights.transforms()
        elif model_name == "resnet50":
            weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2
            self.backbone = torchvision.models.resnet50(weights=weights)
            in_features = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
            self.preprocess = weights.transforms()
        elif model_name == "convnext_tiny":
            weights = torchvision.models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1
            self.backbone = torchvision.models.convnext_tiny(weights=weights)
            in_features = self.backbone.classifier[2].in_features
            self.backbone.classifier = nn.Identity()
            self.preprocess = weights.transforms()

        # Feature projection with normalization
        self.feature_projection = nn.Sequential(
            nn.Linear(in_features, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

    def forward(self, x):
        x = self.preprocess(x)
        x = self.backbone(x)
        x = self.feature_projection(x)
        return x

# Advanced Fusion Model with Feature Preservation
class AdvancedOralCancerFusionModel(LightningModule):
    def __init__(self, cnn_model="efficientnet_b0", num_classes=3, lr=1e-3,
                 weight_decay=1e-4, max_epochs=100, texture_feature_dim=500):
        super().__init__()
        self.save_hyperparameters()

        # CNN feature extractor
        self.cnn_dim = 256
        self.cnn_extractor = EnhancedCNNFeatureExtractor(cnn_model, self.cnn_dim)

        # Texture feature dimension (dynamically calculated)
        self.texture_dim = texture_feature_dim

        # Feature normalization layers to prevent shrinking
        self.cnn_norm = nn.BatchNorm1d(self.cnn_dim)
        self.texture_norm = nn.BatchNorm1d(self.texture_dim)

        # Texture feature enhancement network
        self.texture_enhancer = nn.Sequential(
            nn.Linear(self.texture_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 256),
            nn.BatchNorm1d(256),
            nn.ReLU()
        )

        # Attention mechanism for feature weighting
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=256, num_heads=8, dropout=0.1, batch_first=True
        )

        # Feature fusion with gating mechanism
        self.gate_cnn = nn.Sequential(
            nn.Linear(self.cnn_dim, 128),
            nn.ReLU(),
            nn.Linear(128, self.cnn_dim),
            nn.Sigmoid()
        )

        self.gate_texture = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.Sigmoid()
        )

        # Deep classifier with residual connections
        self.classifier = nn.Sequential(
            nn.Linear(self.cnn_dim + 256, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

        # Loss functions
        self.loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)

        # Metrics storage
        self.training_step_outputs = []
        self.validation_step_outputs = []
        self.test_step_outputs = []
        self.train_start_time = None
        self.test_start_time = None

    def forward(self, image, texture_features):
        # Extract and normalize CNN features
        cnn_features = self.cnn_extractor(image)
        cnn_features = self.cnn_norm(cnn_features)

        # Normalize and enhance texture features
        texture_features = texture_features.to(cnn_features.device)
        texture_features = self.texture_norm(texture_features)
        enhanced_texture = self.texture_enhancer(texture_features)

        # Apply cross-attention between CNN and texture features
        cnn_unsqueeze = cnn_features.unsqueeze(1)
        texture_unsqueeze = enhanced_texture.unsqueeze(1)

        attended_cnn, _ = self.cross_attention(
            cnn_unsqueeze, texture_unsqueeze, texture_unsqueeze
        )
        attended_cnn = attended_cnn.squeeze(1)

        # Apply gating mechanism to preserve important features
        gated_cnn = cnn_features * self.gate_cnn(cnn_features) + attended_cnn * 0.5
        gated_texture = enhanced_texture * self.gate_texture(enhanced_texture)

        # Concatenate features (preserving both)
        fused_features = torch.cat([gated_cnn, gated_texture], dim=1)

        # Classification
        output = self.classifier(fused_features)
        return output

    def on_train_start(self):
        self.train_start_time = time.time()

    def training_step(self, batch, batch_idx):
        image, texture_features, label = batch
        logits = self(image, texture_features)
        loss = self.loss_fn(logits, label)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == label).float().mean()

        self.log("train_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("train_acc", acc, on_step=False, on_epoch=True, prog_bar=True)

        self.training_step_outputs.append({
            'loss': loss,
            'preds': preds.cpu(),
            'labels': label.cpu(),
            'probs': F.softmax(logits, dim=1).cpu()
        })

        return loss

    def on_train_epoch_end(self):
        all_preds = torch.cat([x['preds'] for x in self.training_step_outputs])
        all_labels = torch.cat([x['labels'] for x in self.training_step_outputs])

        epoch_acc = accuracy_score(all_labels, all_preds)
        self.log("train_epoch_acc", epoch_acc, prog_bar=True)

        self.training_step_outputs.clear()

    def validation_step(self, batch, batch_idx):
        image, texture_features, label = batch
        logits = self(image, texture_features)
        loss = self.loss_fn(logits, label)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == label).float().mean()

        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val_acc", acc, on_step=False, on_epoch=True, prog_bar=True)

        self.validation_step_outputs.append({
            'loss': loss,
            'preds': preds.cpu(),
            'labels': label.cpu(),
            'probs': F.softmax(logits, dim=1).cpu()
        })

        return loss

    def on_validation_epoch_end(self):
        all_preds = torch.cat([x['preds'] for x in self.validation_step_outputs])
        all_labels = torch.cat([x['labels'] for x in self.validation_step_outputs])

        epoch_acc = accuracy_score(all_labels, all_preds)
        self.log("val_epoch_acc", epoch_acc, prog_bar=True)

        self.validation_step_outputs.clear()

    def on_test_start(self):
        self.test_start_time = time.time()

    def test_step(self, batch, batch_idx):
        image, texture_features, label = batch
        logits = self(image, texture_features)
        loss = self.loss_fn(logits, label)

        preds = torch.argmax(logits, dim=1)

        self.test_step_outputs.append({
            'loss': loss,
            'preds': preds.cpu(),
            'labels': label.cpu(),
            'probs': F.softmax(logits, dim=1).cpu()
        })

        return loss

    def on_test_epoch_end(self):
        all_preds = torch.cat([x['preds'] for x in self.test_step_outputs])
        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])
        all_probs = torch.cat([x['probs'] for x in self.test_step_outputs])

        # Calculate comprehensive metrics
        test_acc = accuracy_score(all_labels, all_preds)
        cm = confusion_matrix(all_labels, all_preds)

        # For multi-class, calculate metrics per class and average
        precision = precision_score(all_labels, all_preds, average='weighted')
        recall = recall_score(all_labels, all_preds, average='weighted')
        f1 = f1_score(all_labels, all_preds, average='weighted')

        # Calculate AUC for multi-class
        try:
            if len(np.unique(all_labels)) > 2:
                auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')
            else:
                auc = roc_auc_score(all_labels, all_probs[:, 1])
        except:
            auc = 0.0

        # Calculate confusion matrix values
        if len(np.unique(all_labels)) == 2:
            tn, fp, fn, tp = cm.ravel()
        else:
            # For multi-class, sum the confusion matrix
            tp = np.diag(cm).sum()
            fp = cm.sum(axis=0).sum() - tp
            fn = cm.sum(axis=1).sum() - tp
            tn = cm.sum() - tp - fp - fn

        test_time = time.time() - self.test_start_time

        # Store results
        self.test_results = {
            'test_accuracy': test_acc,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc': auc,
            'true_positive': int(tp),
            'true_negative': int(tn),
            'false_positive': int(fp),
            'false_negative': int(fn),
            'test_time': test_time,
            'confusion_matrix': cm
        }

        self.test_step_outputs.clear()

        print(f"\nTest Results:")
        print(f"Accuracy: {test_acc:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        print(f"Confusion Matrix:\n{cm}")

    def configure_optimizers(self):
        # Different learning rates for different parts
        params = [
            {'params': self.cnn_extractor.parameters(), 'lr': self.hparams.lr * 0.1},
            {'params': self.texture_enhancer.parameters(), 'lr': self.hparams.lr},
            {'params': self.classifier.parameters(), 'lr': self.hparams.lr}
        ]

        optimizer = torch.optim.AdamW(params, weight_decay=self.hparams.weight_decay)

        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=self.hparams.lr,
            total_steps=self.trainer.estimated_stepping_batches,
            pct_start=0.3,
            anneal_strategy='cos'
        )

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step",
                "frequency": 1
            }
        }

# Metrics Logger Class
class MetricsLogger:
    def __init__(self, save_path="./"):
        self.save_path = save_path
        self.metrics_df = pd.DataFrame()

    def log_experiment(self, model_name, seed, train_time, test_time,
                      train_acc, test_acc, tp, tn, fp, fn,
                      precision, recall, f1, auc):

        new_row = pd.DataFrame({
            'timestamp': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
            'model_name': [model_name],
            'seed': [seed],
            'train_time_seconds': [train_time],
            'test_time_seconds': [test_time],
            'train_accuracy': [train_acc],
            'test_accuracy': [test_acc],
            'true_positive': [tp],
            'true_negative': [tn],
            'false_positive': [fp],
            'false_negative': [fn],
            'precision': [precision],
            'recall': [recall],
            'f1_score': [f1],
            'auc_roc': [auc]
        })

        self.metrics_df = pd.concat([self.metrics_df, new_row], ignore_index=True)

        # Save to CSV
        csv_path = os.path.join(self.save_path, 'experiment_metrics.csv')
        self.metrics_df.to_csv(csv_path, index=False)
        print(f"Metrics saved to {csv_path}")

        return self.metrics_df

# Enhanced Training Function
def train_enhanced_model(model_name="efficientnet_b0", batch_size=32, max_epochs=100):
    print(f"\n{'='*50}")
    print(f"Training Enhanced Fusion Model")
    print(f"Model: {model_name}, Seed: {SEED}")
    print(f"{'='*50}\n")

    # Paths
    train_path = "./datasets/train.json"
    val_path = "./datasets/val.json"
    test_path = "./datasets/test.json"

    # Enhanced transforms with more augmentation
    train_transform = T.Compose([
        T.Resize((256, 256), antialias=True),
        T.RandomCrop(224),
        T.RandomHorizontalFlip(p=0.5),
        T.RandomVerticalFlip(p=0.3),
        T.RandomRotation(20),
        T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15),
        T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        T.RandomPerspective(distortion_scale=0.2, p=0.3),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        T.RandomErasing(p=0.2)
    ])

    val_transform = T.Compose([
        T.Resize((256, 256), antialias=True),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Create datasets
    train_dataset = EnhancedFusionDataset(train_path, train_transform)
    val_dataset = EnhancedFusionDataset(val_path, val_transform)
    test_dataset = EnhancedFusionDataset(test_path, val_transform)

    # Calculate texture feature dimension
    sample_img, sample_texture, _ = train_dataset[0]
    texture_dim = sample_texture.shape[0]

    print(f"Dataset Statistics:")
    print(f"  Train: {len(train_dataset)} samples")
    print(f"  Val: {len(val_dataset)} samples")
    print(f"  Test: {len(test_dataset)} samples")
    print(f"  Categories: {train_dataset.get_category_names()}")
    print(f"  Texture features dimension: {texture_dim}")

    # Data loaders with optimized settings
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )

    # Initialize model
    model = AdvancedOralCancerFusionModel(
        cnn_model=model_name,
        num_classes=len(train_dataset.get_category_names()),
        lr=1e-3,
        weight_decay=1e-4,
        max_epochs=max_epochs,
        texture_feature_dim=texture_dim
    )

    # Callbacks
    checkpoint_callback = ModelCheckpoint(
        monitor="val_loss",
        mode="min",
        save_top_k=3,
        filename=f"{model_name}-{{epoch:02d}}-{{val_loss:.3f}}-{{val_acc:.3f}}",
        save_last=True
    )

    early_stopping = EarlyStopping(
        monitor="val_loss",
        mode="min",
        patience=15,
        verbose=True,
        min_delta=0.001
    )

    lr_monitor = LearningRateMonitor(logging_interval='epoch')

    # Trainer with optimized settings
    trainer = Trainer(
        max_epochs=max_epochs,
        callbacks=[checkpoint_callback, early_stopping, lr_monitor],
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        devices=1,
        precision="16-mixed" if torch.cuda.is_available() else 32,
        gradient_clip_val=1.0,
        accumulate_grad_batches=2,
        deterministic=True,
        enable_progress_bar=True,
        log_every_n_steps=10
    )

    # Training
    train_start = time.time()
    trainer.fit(model, train_loader, val_loader)
    train_time = time.time() - train_start

    # Get training accuracy from last epoch
    train_acc = trainer.callback_metrics.get('train_epoch_acc', 0.0)

    # Testing
    test_results = trainer.test(model, test_loader, ckpt_path='best')

    # Extract metrics
    test_metrics = model.test_results

    return {
        'model': model,
        'trainer': trainer,
        'train_time': train_time,
        'train_acc': float(train_acc),
        'test_metrics': test_metrics
    }

# Main experiment runner with multiple models
def run_experiments():
    """Run experiments with different CNN backbones and log all metrics"""

    # Initialize metrics logger
    logger = MetricsLogger(save_path="./")

    # Models to test
    models_to_test = [
        ("efficientnet_b0", 32, 100),
        ("resnet50", 24, 100),
        ("convnext_tiny", 16, 100)
    ]

    all_results = []

    for model_name, batch_size, epochs in models_to_test:
        print(f"\n{'='*60}")
        print(f"EXPERIMENT: {model_name.upper()}")
        print(f"{'='*60}")

        try:
            # Train model
            results = train_enhanced_model(
                model_name=model_name,
                batch_size=batch_size,
                max_epochs=epochs
            )

            # Log metrics to CSV
            logger.log_experiment(
                model_name=model_name,
                seed=SEED,
                train_time=results['train_time'],
                test_time=results['test_metrics']['test_time'],
                train_acc=results['train_acc'],
                test_acc=results['test_metrics']['test_accuracy'],
                tp=results['test_metrics']['true_positive'],
                tn=results['test_metrics']['true_negative'],
                fp=results['test_metrics']['false_positive'],
                fn=results['test_metrics']['false_negative'],
                precision=results['test_metrics']['precision'],
                recall=results['test_metrics']['recall'],
                f1=results['test_metrics']['f1_score'],
                auc=results['test_metrics']['auc']
            )

            # Store results
            all_results.append({
                'model': model_name,
                'results': results['test_metrics']
            })

            # Print summary
            print(f"\n{model_name.upper()} SUMMARY:")
            print(f"  Train Time: {results['train_time']:.2f} seconds")
            print(f"  Test Time: {results['test_metrics']['test_time']:.2f} seconds")
            print(f"  Train Accuracy: {results['train_acc']:.4f}")
            print(f"  Test Accuracy: {results['test_metrics']['test_accuracy']:.4f}")
            print(f"  Precision: {results['test_metrics']['precision']:.4f}")
            print(f"  Recall: {results['test_metrics']['recall']:.4f}")
            print(f"  F1-Score: {results['test_metrics']['f1_score']:.4f}")
            print(f"  AUC-ROC: {results['test_metrics']['auc']:.4f}")

        except Exception as e:
            print(f"Error training {model_name}: {str(e)}")
            continue

    # Print final comparison
    print(f"\n{'='*60}")
    print("FINAL COMPARISON")
    print(f"{'='*60}")

    comparison_df = pd.DataFrame([
        {
            'Model': r['model'],
            'Test Accuracy': f"{r['results']['test_accuracy']:.4f}",
            'Precision': f"{r['results']['precision']:.4f}",
            'Recall': f"{r['results']['recall']:.4f}",
            'F1-Score': f"{r['results']['f1_score']:.4f}",
            'AUC': f"{r['results']['auc']:.4f}"
        }
        for r in all_results
    ])

    print(comparison_df.to_string(index=False))

    # Save comparison
    comparison_df.to_csv('./model_comparison.csv', index=False)

    return all_results, logger.metrics_df

# Custom loss function for handling class imbalance
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ce = nn.CrossEntropyLoss(reduction='none')

    def forward(self, inputs, targets):
        ce_loss = self.ce(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

# Advanced data augmentation class
class MixUpAugmentation:
    def __init__(self, alpha=0.2):
        self.alpha = alpha

    def __call__(self, images, labels):
        batch_size = images.size(0)
        indices = torch.randperm(batch_size)

        lam = np.random.beta(self.alpha, self.alpha)

        mixed_images = lam * images + (1 - lam) * images[indices]
        labels_a, labels_b = labels, labels[indices]

        return mixed_images, labels_a, labels_b, lam

# Ensemble model for better accuracy
class EnsembleFusionModel(LightningModule):
    def __init__(self, models_configs, num_classes=3):
        super().__init__()
        self.models = nn.ModuleList()

        for config in models_configs:
            model = AdvancedOralCancerFusionModel(
                cnn_model=config['name'],
                num_classes=num_classes,
                lr=config['lr'],
                weight_decay=config['weight_decay'],
                texture_feature_dim=config['texture_dim']
            )
            self.models.append(model)

        self.ensemble_weights = nn.Parameter(torch.ones(len(models_configs)) / len(models_configs))

    def forward(self, image, texture_features):
        outputs = []
        for model in self.models:
            output = model(image, texture_features)
            outputs.append(F.softmax(output, dim=1))

        # Weighted average of predictions
        weighted_output = sum(w * out for w, out in zip(F.softmax(self.ensemble_weights, dim=0), outputs))
        return torch.log(weighted_output + 1e-8)  # Log for numerical stability

# Feature importance analysis
def analyze_feature_importance(model, test_loader, device='cuda'):
    """Analyze the importance of texture vs CNN features"""
    model.eval()
    model = model.to(device)

    texture_importance = []
    cnn_importance = []

    with torch.no_grad():
        for batch in test_loader:
            images, texture_features, labels = batch
            images = images.to(device)
            texture_features = texture_features.to(device)
            labels = labels.to(device)

            # Normal prediction
            normal_output = model(images, texture_features)
            normal_probs = F.softmax(normal_output, dim=1)

            # Prediction with zeroed texture features
            zero_texture = torch.zeros_like(texture_features)
            texture_zero_output = model(images, zero_texture)
            texture_zero_probs = F.softmax(texture_zero_output, dim=1)

            # Prediction with random CNN features (baseline)
            random_images = torch.randn_like(images)
            cnn_random_output = model(random_images, texture_features)
            cnn_random_probs = F.softmax(cnn_random_output, dim=1)

            # Calculate importance as difference from normal prediction
            texture_imp = (normal_probs - texture_zero_probs).abs().mean().item()
            cnn_imp = (normal_probs - cnn_random_probs).abs().mean().item()

            texture_importance.append(texture_imp)
            cnn_importance.append(cnn_imp)

    avg_texture_importance = np.mean(texture_importance)
    avg_cnn_importance = np.mean(cnn_importance)

    print(f"\nFeature Importance Analysis:")
    print(f"  Texture Features: {avg_texture_importance:.4f}")
    print(f"  CNN Features: {avg_cnn_importance:.4f}")
    print(f"  Ratio (Texture/CNN): {avg_texture_importance/avg_cnn_importance:.2f}")

    return avg_texture_importance, avg_cnn_importance

# Visualization function for results
def visualize_results(metrics_df):
    """Create visualizations of the experimental results"""
    import matplotlib.pyplot as plt

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # Accuracy comparison
    axes[0, 0].bar(metrics_df['model_name'], metrics_df['test_accuracy'])
    axes[0, 0].set_title('Test Accuracy by Model')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_ylim([0, 1])

    # Precision-Recall comparison
    axes[0, 1].scatter(metrics_df['recall'], metrics_df['precision'], s=100)
    for i, model in enumerate(metrics_df['model_name']):
        axes[0, 1].annotate(model, (metrics_df['recall'].iloc[i], metrics_df['precision'].iloc[i]))
    axes[0, 1].set_xlabel('Recall')
    axes[0, 1].set_ylabel('Precision')
    axes[0, 1].set_title('Precision vs Recall')

    # F1 Score comparison
    axes[0, 2].bar(metrics_df['model_name'], metrics_df['f1_score'])
    axes[0, 2].set_title('F1 Score by Model')
    axes[0, 2].set_ylabel('F1 Score')
    axes[0, 2].set_ylim([0, 1])

    # Training time comparison
    axes[1, 0].bar(metrics_df['model_name'], metrics_df['train_time_seconds']/60)
    axes[1, 0].set_title('Training Time by Model')
    axes[1, 0].set_ylabel('Time (minutes)')

    # AUC comparison
    axes[1, 1].bar(metrics_df['model_name'], metrics_df['auc_roc'])
    axes[1, 1].set_title('AUC-ROC by Model')
    axes[1, 1].set_ylabel('AUC')
    axes[1, 1].set_ylim([0, 1])

    # Confusion metrics
    metrics = ['true_positive', 'true_negative', 'false_positive', 'false_negative']
    width = 0.2
    x = np.arange(len(metrics_df))

    for i, metric in enumerate(metrics):
        axes[1, 2].bar(x + i*width, metrics_df[metric], width, label=metric)

    axes[1, 2].set_xlabel('Model')
    axes[1, 2].set_xticks(x + width * 1.5)
    axes[1, 2].set_xticklabels(metrics_df['model_name'])
    axes[1, 2].set_title('Confusion Matrix Metrics')
    axes[1, 2].legend()

    plt.tight_layout()
    plt.savefig('./experiment_results.png', dpi=300, bbox_inches='tight')
    plt.show()

# Main execution
if __name__ == "__main__":
    print("="*60)
    print("ENHANCED ORAL CANCER DETECTION WITH FUSION MODEL")
    print(f"Seed: {SEED}")
    print(f"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
    print("="*60)

    # Run all experiments
    results, metrics_df = run_experiments()

    # Visualize results
    if len(metrics_df) > 0:
        visualize_results(metrics_df)

        # Print final summary
        print("\n" + "="*60)
        print("EXPERIMENT COMPLETE")
        print("="*60)
        print(f"Results saved to: ./experiment_metrics.csv")
        print(f"Comparison saved to: ./model_comparison.csv")
        print(f"Visualizations saved to: ./experiment_results.png")

        # Best model
        best_model_idx = metrics_df['test_accuracy'].idxmax()
        best_model = metrics_df.iloc[best_model_idx]
        print(f"\nBest Model: {best_model['model_name']}")
        print(f"  Test Accuracy: {best_model['test_accuracy']:.4f}")
        print(f"  F1-Score: {best_model['f1_score']:.4f}")
        print(f"  AUC-ROC: {best_model['auc_roc']:.4f}")